{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning (MARL)\n",
    "\n",
    "This tutorial demonstrates how to use Tianshou for multi-agent reinforcement learning scenarios. We'll explore different MARL paradigms and implement a practical example using the Tic-Tac-Toe game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARL Paradigms\n",
    "\n",
    "Tianshou supports three fundamental types of multi-agent reinforcement learning paradigms:\n",
    "\n",
    "1. **Simultaneous move**: All agents take their actions at each timestep simultaneously (e.g., MOBA games)\n",
    "2. **Cyclic move**: Agents take actions sequentially in turns (e.g., Go)\n",
    "3. **Conditional move**: The environment conditionally selects which agent acts at each timestep (e.g., [Pig Game](https://en.wikipedia.org/wiki/Pig_(dice_game)))\n",
    "\n",
    "Our approach addresses these multi-agent RL problems by converting them into traditional single-agent RL formulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting MARL to Single-Agent RL\n",
    "\n",
    "### Simultaneous Move\n",
    "\n",
    "For simultaneous-move scenarios, the solution is straightforward: we add an extra `num_agents` dimension to the state, action, and reward tensors. No other modifications are necessary.\n",
    "\n",
    "### Cyclic and Conditional Move\n",
    "\n",
    "Both cyclic and conditional move scenarios can be unified into a single framework. At each timestep, the environment selects an agent identified by `agent_id` to act. Since multiple agents are typically wrapped into a single object (the \"abstract agent\"), we pass the `agent_id` to this abstract agent, which then delegates the action to the appropriate specific agent.\n",
    "\n",
    "Additionally, in multi-agent RL, the set of legal actions often varies across timesteps (as in Go). Therefore, the environment must also provide a legal action mask to the abstract agent. This mask is a boolean array where `True` indicates available actions and `False` indicates illegal actions at the current timestep.\n",
    "\n",
    "<div style=\"text-align: center; padding: 1rem;\">\n",
    "<img src=\"../_static/images/marl.png\" style=\"height: 300px; padding-bottom: 1rem;\"><br>\n",
    "The abstract agent framework for multi-agent RL\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Formulation\n",
    "\n",
    "This architecture leads to the following formulation of multi-agent RL:\n",
    "\n",
    "```python\n",
    "act = policy(state, agent_id, mask)\n",
    "(next_state, next_agent_id, next_mask), reward = env.step(act)\n",
    "```\n",
    "\n",
    "By constructing an augmented state `state_ = (state, agent_id, mask)`, we can reduce this to the standard single-agent RL formulation:\n",
    "\n",
    "```python\n",
    "act = policy(state_)\n",
    "next_state_, reward = env.step(act)\n",
    "```\n",
    "\n",
    "Following this principle, we'll implement a Q-learning algorithm to play [Tic-Tac-Toe](https://en.wikipedia.org/wiki/Tic-tac-toe) against a random opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PettingZoo Integration\n",
    "\n",
    "Tianshou is fully compatible with [PettingZoo](https://pettingzoo.farama.org/) environments for multi-agent RL. While Tianshou doesn't directly provide specialized MARL facilities, it offers a flexible framework that can be adapted to various MARL scenarios.\n",
    "\n",
    "For comprehensive tutorials on using Tianshou with PettingZoo, refer to:\n",
    "\n",
    "* [Beginner Tutorial](https://pettingzoo.farama.org/tutorials/tianshou/beginner/)\n",
    "* [Intermediate Tutorial](https://pettingzoo.farama.org/tutorials/tianshou/intermediate/)\n",
    "* [Advanced Tutorial](https://pettingzoo.farama.org/tutorials/tianshou/advanced/)\n",
    "\n",
    "In this tutorial, we'll demonstrate how to use Tianshou in a multi-agent setting where only one agent is trained while the other uses a fixed random policy. You can then use this as a blueprint to replace the random policy with another trainable agent.\n",
    "\n",
    "Specifically, we'll train an agent to play Tic-Tac-Toe against a random opponent:\n",
    "\n",
    "<div style=\"text-align: center; padding: 1rem;\">\n",
    "<img src=\"../_static/images/tic-tac-toe.png\" style=\"padding-bottom: 1rem;\"><br>\n",
    "Tic-Tac-Toe game board\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Tic-Tac-Toe Environment\n",
    "\n",
    "The complete scripts are located in `test/pettingzoo/`. Tianshou provides the `PettingZooEnv` wrapper class that can wrap any PettingZoo environment. Let's explore the 3×3 Tic-Tac-Toe environment provided by PettingZoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.classic import tictactoe_v3  # the Tic-Tac-Toe environment\n",
    "\n",
    "from tianshou.env import PettingZooEnv  # wrapper for PettingZoo environments\n",
    "\n",
    "# Initialize the environment\n",
    "# The board has 3 rows and 3 columns (9 positions total)\n",
    "# Players place 'X' and 'O' alternately on the board\n",
    "# The first player to get 3 consecutive marks wins\n",
    "env = PettingZooEnv(tictactoe_v3.env(render_mode=\"human\"))\n",
    "obs = env.reset()\n",
    "env.render()  # render the empty board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows an empty 3×3 board:\n",
    "\n",
    "```\n",
    "board (step 0):\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the observation structure\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Observation Space\n",
    "\n",
    "The observation returned by the environment is a dictionary with three keys:\n",
    "\n",
    "- **`agent_id`**: The identifier of the currently acting agent (e.g., `'player_1'` or `'player_2'`)\n",
    "\n",
    "- **`obs`**: The actual environment observation. For Tic-Tac-Toe, this is a numpy array with shape `(3, 3, 2)`:\n",
    "  - For `player_1`: The first 3×3 plane represents X placements, the second plane represents O placements\n",
    "  - For `player_2`: The planes are swapped (O in first plane, X in second)\n",
    "  - Each cell contains either 0 (empty/not placed) or 1 (mark placed)\n",
    "\n",
    "- **`mask`**: A boolean array indicating legal actions at the current timestep. For Tic-Tac-Toe, index `i` corresponds to position `(i // 3, i % 3)` on the board. If `mask[i] == True`, the player can place their mark at that position. Initially, all positions are available, so all mask values are `True`.\n",
    "\n",
    "> **Note**: The mask representation is flexible and works for both discrete and continuous action spaces. While we use a boolean array here, you could also use action spaces like `gymnasium.spaces.Discrete` or `gymnasium.spaces.Box` to represent available actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a Few Steps\n",
    "\n",
    "Let's play a couple of moves to understand the environment dynamics better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Take an action (place mark at position 0 - top-left corner)\n",
    "action = 0  # action can be an integer or a numpy array with one element\n",
    "obs, reward, done, truncated, info = env.step(action)  # follows the Gymnasium API\n",
    "\n",
    "print(\"Observation after first move:\")\n",
    "print(obs)\n",
    "\n",
    "# Examine the reward structure\n",
    "# Reward has two items (one for each player): 1 for win, -1 for loss, 0 otherwise\n",
    "print(f\"\\nReward: {reward}\")\n",
    "\n",
    "# Check if the game is over\n",
    "print(f\"Done: {done}\")\n",
    "\n",
    "# Info is typically an empty dict in Tic-Tac-Toe but may contain useful information in other environments\n",
    "print(f\"Info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that after the first move:\n",
    "- The `agent_id` switches to `'player_2'`\n",
    "- The observation array shows the X placement in the first position\n",
    "- The mask now has `False` at index 0 (that position is occupied)\n",
    "- The reward is `[0, 0]` (no winner yet)\n",
    "- The game continues (`done = False`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Termination\n",
    "\n",
    "An interesting detail: the game terminates when only one empty position remains, rather than when the board is completely full. This is because a player with only one available position has no meaningful choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue playing (positions: 3, 1, 4, then 2)\n",
    "# ... (intermediate moves omitted for brevity)\n",
    "\n",
    "# Final move where player_1 wins\n",
    "obs, reward, done, info = env.step(2)\n",
    "print(f\"Final state - Reward: {reward}, Done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final board shows player_1 (X) winning with three consecutive marks:\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "The reward is `[1, -1]` indicating player_1 wins (+1) and player_2 loses (-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agents\n",
    "\n",
    "Now that we understand the environment, let's start by watching two random agents play against each other.\n",
    "\n",
    "Tianshou provides built-in classes for multi-agent learning. The key components are:\n",
    "\n",
    "- **`RandomPolicy`**: A policy that randomly selects actions\n",
    "- **`MultiAgentPolicyManager`**: Manages multiple agent policies and delegates actions to the appropriate agent based on `agent_id`\n",
    "\n",
    "<div style=\"text-align: center; padding: 1rem;\">\n",
    "<img src=\"../_static/images/marl.png\" style=\"height: 300px; padding-bottom: 1rem;\"><br>\n",
    "The relationship between MultiAgentPolicyManager and individual agent policies\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.algorithm.algorithm_base import RandomActionPolicy\n",
    "from tianshou.algorithm.multiagent.marl import MultiAgentPolicy\n",
    "from tianshou.data import Collector\n",
    "from tianshou.env import DummyVectorEnv\n",
    "\n",
    "# Create a multi-agent policy with two random agents\n",
    "policy = MultiAgentPolicy(\n",
    "    {\n",
    "        \"a\": RandomActionPolicy(action_space=env.action_space),\n",
    "        \"b\": RandomActionPolicy(action_space=env.action_space),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Vectorize the environment for the collector\n",
    "env = DummyVectorEnv([lambda: env])\n",
    "\n",
    "# Create a collector to gather trajectories\n",
    "collector = Collector(policy, env)\n",
    "\n",
    "# Collect and visualize one episode\n",
    "result = collector.collect(n_episode=1, render=0.1, reset_before_collect=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see the game progress step by step. Here's an example of the final moves:\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  O  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  O  |  -  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  X  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  O  |  -  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "Random agents perform poorly. In the game above, although agent 2 eventually wins, a smart agent 1 would have won immediately by placing an X at position (1, 1) (center of middle row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Agent Against a Random Opponent\n",
    "\n",
    "Now let's train an intelligent agent! We'll use Deep Q-Network (DQN) to learn optimal play against a random opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup\n",
    "\n",
    "First, let's import all necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.algorithm import (\n",
    "    BasePolicy,\n",
    "    DQNPolicy,\n",
    "    MultiAgentPolicyManager,\n",
    "    RandomPolicy,\n",
    ")\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import MLPActor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Let's define the hyperparameters for our training experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=1626)\n",
    "    parser.add_argument(\"--eps-test\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--eps-train\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=20000)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        help=\"Discount factor (smaller values favor earlier wins)\",\n",
    "    )\n",
    "    parser.add_argument(\"--n-step\", type=int, default=3)\n",
    "    parser.add_argument(\"--target-update-freq\", type=int, default=320)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=50)\n",
    "    parser.add_argument(\"--epoch_num_steps\", type=int, default=1000)\n",
    "    parser.add_argument(\"--collection_step_num_env_steps\", type=int, default=10)\n",
    "    parser.add_argument(\"--update-per-step\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--hidden-sizes\", type=int, nargs=\"*\", default=[128, 128, 128, 128])\n",
    "    parser.add_argument(\"--num_train_envs\", type=int, default=10)\n",
    "    parser.add_argument(\"--num_test_envs\", type=int, default=10)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "    parser.add_argument(\"--render\", type=float, default=0.1)\n",
    "    parser.add_argument(\n",
    "        \"--win-rate\",\n",
    "        type=float,\n",
    "        default=0.6,\n",
    "        help=\"Target winning rate (optimal policy achieves ~0.7)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--watch\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Skip training and watch pre-trained models play\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--agent-id\", type=int, default=2, help=\"The learned agent plays as agent {1 or 2}\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume-path\", type=str, default=\"\", help=\"Path to pre-trained agent .pth file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opponent-path\", type=str, default=\"\", help=\"Path to pre-trained opponent .pth file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "\n",
    "def get_args() -> argparse.Namespace:\n",
    "    parser = get_parser()\n",
    "    return parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Setup\n",
    "\n",
    "The `get_agents` function creates and configures our agents:\n",
    "\n",
    "- **Action Model**: We use `MLPActor`, a multi-layer perceptron with ReLU activations\n",
    "- **Learning Agent**: A `DQNPolicy` that selects actions based on both the action mask and Q-values\n",
    "- **Opponent**: Either a `RandomPolicy` that randomly chooses legal actions, or a pre-trained `DQNPolicy` for self-play\n",
    "\n",
    "Both agents are managed by `MultiAgentPolicyManager`, which:\n",
    "- Calls the correct agent based on `agent_id` in the observation\n",
    "- Dispatches data to each agent according to their `agent_id`\n",
    "- Makes each agent perceive the environment as a single-agent problem\n",
    "\n",
    "<div style=\"text-align: center; padding: 1rem;\">\n",
    "<img src=\"../_static/images/marl.png\" style=\"height: 300px; padding-bottom: 1rem;\"><br>\n",
    "How MultiAgentPolicyManager coordinates agent policies\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(render_mode=None):\n",
    "    \"\"\"Create a Tic-Tac-Toe environment.\"\"\"\n",
    "    return PettingZooEnv(tictactoe_v3.env(render_mode=render_mode))\n",
    "\n",
    "\n",
    "def get_agents(\n",
    "    args: argparse.Namespace = get_args(),\n",
    "    agent_learn: BasePolicy | None = None,\n",
    "    agent_opponent: BasePolicy | None = None,\n",
    "    optim: torch.optim.Optimizer | None = None,\n",
    ") -> tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \"\"\"Create or load agents for training.\"\"\"\n",
    "    env = get_env()\n",
    "    observation_space = (\n",
    "        env.observation_space[\"observation\"]\n",
    "        if isinstance(env.observation_space, gym.spaces.Dict)\n",
    "        else env.observation_space\n",
    "    )\n",
    "    args.state_shape = observation_space.shape or observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "\n",
    "    if agent_learn is None:\n",
    "        # Create the neural network model\n",
    "        net = MLPActor(\n",
    "            args.state_shape, args.action_shape, hidden_sizes=args.hidden_sizes, device=args.device\n",
    "        ).to(args.device)\n",
    "\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "\n",
    "        # Create DQN policy for the learning agent\n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            gamma=args.gamma,\n",
    "            action_space=env.action_space,\n",
    "            estimate_space=args.n_step,\n",
    "            target_update_freq=args.target_update_freq,\n",
    "        )\n",
    "\n",
    "        if args.resume_path:\n",
    "            agent_learn.load_state_dict(torch.load(args.resume_path))\n",
    "\n",
    "    if agent_opponent is None:\n",
    "        if args.opponent_path:\n",
    "            # Load a pre-trained opponent for self-play\n",
    "            agent_opponent = deepcopy(agent_learn)\n",
    "            agent_opponent.load_state_dict(torch.load(args.opponent_path))\n",
    "        else:\n",
    "            # Use a random opponent\n",
    "            agent_opponent = RandomPolicy(action_space=env.action_space)\n",
    "\n",
    "    # Arrange agents based on which player position the learning agent takes\n",
    "    if args.agent_id == 1:\n",
    "        agents = [agent_learn, agent_opponent]\n",
    "    else:\n",
    "        agents = [agent_opponent, agent_learn]\n",
    "\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "The training procedure follows the standard Tianshou workflow, similar to single-agent DQN training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    args: argparse.Namespace = get_args(),\n",
    "    agent_learn: BasePolicy | None = None,\n",
    "    agent_opponent: BasePolicy | None = None,\n",
    "    optim: torch.optim.Optimizer | None = None,\n",
    ") -> tuple[dict, BasePolicy]:\n",
    "    \"\"\"Train the agent using DQN.\"\"\"\n",
    "    # ======== Environment Setup =========\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(args.num_train_envs)])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(args.num_test_envs)])\n",
    "\n",
    "    # Set random seeds for reproducibility\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "\n",
    "    # ======== Agent Setup =========\n",
    "    policy, optim, agents = get_agents(\n",
    "        args, agent_learn=agent_learn, agent_opponent=agent_opponent, optim=optim\n",
    "    )\n",
    "\n",
    "    # ======== Collector Setup =========\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "        exploration_noise=True,\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "\n",
    "    # Collect initial random samples\n",
    "    train_collector.collect(n_step=args.batch_size * args.num_train_envs)\n",
    "\n",
    "    # ======== Logging Setup =========\n",
    "    log_path = os.path.join(args.logdir, \"tic_tac_toe\", \"dqn\")\n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    # ======== Callback Functions =========\n",
    "    def save_best_fn(policy):\n",
    "        \"\"\"Save the best performing policy.\"\"\"\n",
    "        model_save_path = getattr(\n",
    "            args, \"model_save_path\", os.path.join(args.logdir, \"tic_tac_toe\", \"dqn\", \"policy.pth\")\n",
    "        )\n",
    "        torch.save(policy.policies[agents[args.agent_id - 1]].state_dict(), model_save_path)\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        \"\"\"Stop training when target win rate is achieved.\"\"\"\n",
    "        return mean_rewards >= args.win_rate\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        \"\"\"Set exploration rate for training.\"\"\"\n",
    "        policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_train)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        \"\"\"Set exploration rate for testing.\"\"\"\n",
    "        policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        \"\"\"Extract the reward for our learning agent.\"\"\"\n",
    "        return rews[:, args.agent_id - 1]\n",
    "\n",
    "    # ======== Trainer =========\n",
    "    result = OffpolicyTrainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.epoch_num_steps,\n",
    "        args.collection_step_num_env_steps,\n",
    "        args.num_test_envs,\n",
    "        args.batch_size,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=args.update_per_step,\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "    ).run()\n",
    "\n",
    "    return result, policy.policies[agents[args.agent_id - 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "This function allows us to watch a trained agent play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch(\n",
    "    args: argparse.Namespace = get_args(),\n",
    "    agent_learn: BasePolicy | None = None,\n",
    "    agent_opponent: BasePolicy | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Watch a pre-trained agent play.\"\"\"\n",
    "    env = get_env(render_mode=\"human\")\n",
    "    env = DummyVectorEnv([lambda: env])\n",
    "    policy, optim, agents = get_agents(args, agent_learn=agent_learn, agent_opponent=agent_opponent)\n",
    "    policy.eval()\n",
    "    policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "    collector = Collector(policy, env, exploration_noise=True)\n",
    "    result = collector.collect(n_episode=1, render=args.render)\n",
    "    rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "    print(f\"Final reward: {rews[:, args.agent_id - 1].mean()}, Episode length: {lens.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Training\n",
    "\n",
    "Now let's train the agent and watch it play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "args = get_args()\n",
    "result, agent = train_agent(args)\n",
    "\n",
    "# Watch the trained agent play\n",
    "watch(args, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "After training for less than a minute, you'll see the agent play against the random opponent. Here's an example game:\n",
    "\n",
    "<details>\n",
    "<summary>Example: Trained Agent vs Random Opponent</summary>\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  O  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  O  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  O  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  O  |  X\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "Final reward: 1.0, length: 8.0\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our trained agent plays as player 2 (O) and wins! The agent has learned the game rules through trial and error, understanding that three consecutive O marks lead to victory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command-Line Usage\n",
    "\n",
    "You can also save the above code as a script (available at `test/pettingzoo/test_tic_tac_toe.py`) and run it from the command line:\n",
    "\n",
    "```bash\n",
    "# Train an agent\n",
    "python test_tic_tac_toe.py\n",
    "```\n",
    "\n",
    "By default, the trained agent is saved to `log/tic_tac_toe/dqn/policy.pth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Play\n",
    "\n",
    "You can make the trained agent play against itself:\n",
    "\n",
    "```bash\n",
    "python test_tic_tac_toe.py --watch \\\n",
    "    --resume-path log/tic_tac_toe/dqn/policy.pth \\\n",
    "    --opponent-path log/tic_tac_toe/dqn/policy.pth\n",
    "```\n",
    "\n",
    "Here's an example of self-play:\n",
    "\n",
    "<details>\n",
    "<summary>Example: Agent Playing Against Itself</summary>\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  -  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  -\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  -  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  -\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  -  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  -  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  -  |  X  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  X  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "```\n",
    "     |     |\n",
    "  X  |  O  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  O  |  X  |  O\n",
    "_____|_____|_____\n",
    "     |     |\n",
    "  X  |  X  |  O\n",
    "     |     |\n",
    "```\n",
    "\n",
    "Final reward: 1.0, length: 8.0\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the trained agent plays well against a random opponent, it's still far from perfect play. The next step would be to implement self-play training, similar to AlphaZero, where the agent continuously improves by playing against increasingly stronger versions of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated how to use Tianshou for training a single agent in a multi-agent reinforcement learning setting. Key takeaways:\n",
    "\n",
    "1. **MARL Paradigms**: Tianshou supports simultaneous, cyclic, and conditional move scenarios\n",
    "2. **Abstraction**: Multi-agent problems can be converted to single-agent RL through clever state augmentation\n",
    "3. **PettingZoo Integration**: Seamless compatibility with PettingZoo environments via `PettingZooEnv`\n",
    "4. **Policy Management**: `MultiAgentPolicyManager` handles agent coordination and data distribution\n",
    "5. **Flexible Framework**: Easy to extend from single-agent training to more complex multi-agent scenarios\n",
    "\n",
    "Tianshou provides a flexible and intuitive framework for reinforcement learning. Experiment with different architectures, training regimes, and opponent strategies to build even more capable agents!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
